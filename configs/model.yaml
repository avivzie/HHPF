# Model configuration for HHPF

# Llama-3 Inference Settings
llama:
  # Model selection (Together AI serverless models)
  model_8b: "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
  model_70b: "meta-llama/Llama-3.3-70B-Instruct-Turbo"
  default_model: "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
  
  # Generation parameters
  max_tokens: 1024
  temperature: 0.8  # For stochastic sampling
  top_p: 0.95
  top_k: 50
  
  # Stochastic sampling for semantic entropy
  num_samples: 5  # Reduced from 10 to 5 (2026-02-04) to speed up generation and reduce API timeouts
  sample_temperature_range: [0.7, 1.0]  # Vary temperature for diversity
  
  # API settings
  api_timeout: 90  # Increased from 60 to 90 seconds for longer generations
  max_retries: 3
  retry_delay: 2  # Base delay for exponential backoff (2s, 4s, 8s)
  rate_limit_delay: 0.5  # seconds between requests
  
  # Logits extraction
  logprobs: 5  # Number of top logits to extract (Together AI max: 5)
  echo: false

# XGBoost Classifier Settings
xgboost:
  # Initial hyperparameters
  max_depth: 6
  learning_rate: 0.05
  n_estimators: 200
  min_child_weight: 1
  subsample: 0.8
  colsample_bytree: 0.8
  gamma: 0
  reg_alpha: 0
  reg_lambda: 1
  
  # Training settings
  objective: "binary:logistic"
  eval_metric: ["auc", "logloss"]
  early_stopping_rounds: 20
  
  # Hyperparameter tuning
  hyperparameter_tuning:
    enabled: true
    method: "optuna"  # or "grid_search"
    n_trials: 100
    cv_folds: 5
    search_space:
      max_depth: [4, 6, 8, 10]
      learning_rate: [0.01, 0.05, 0.1]
      n_estimators: [100, 200, 300]
      min_child_weight: [1, 3, 5]
      subsample: [0.7, 0.8, 0.9]
      colsample_bytree: [0.7, 0.8, 0.9]
  
  # Class imbalance handling
  scale_pos_weight: "auto"  # Automatically calculate based on class distribution

# Cache Settings
cache:
  enabled: true
  cache_dir: "./cache"
  cache_responses: true
  cache_features: true
  cache_models: true
