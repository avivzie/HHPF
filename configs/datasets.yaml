# Dataset-specific configuration for HHPF

datasets:
  medicine:
    name: "Med-HALT"
    domain: "Medicine"
    file_path: "data/raw/med_halt.csv"
    prompt_column: "question"
    answer_column: "correct_answer"
    ground_truth_method: "medical_entity_matching"
    description: "Clinical reasoning questions with ground truth medical answers"
    
  math:
    name: "GSM8K"
    domain: "Math"
    file_path: "data/raw/gsm8k.csv"
    prompt_column: "question"
    answer_column: "answer"
    ground_truth_method: "exact_numerical_match"
    description: "Grade school math word problems with numerical answers"
    
  finance:
    name: "FinanceBench"
    domain: "Finance"
    file_path: "data/raw/financebench_sample_150.csv"
    prompt_column: "question"
    answer_column: "answer"
    ground_truth_method: "numerical_accuracy"
    description: "Financial Q&A with numerical and factual answers"
    tolerance: 0.01  # 1% tolerance for numerical answers
    
  is_agents:
    name: "HalluMix"
    domain: "IS"
    file_path: "data/raw/hallumix.csv"
    prompt_column: "question"
    answer_column: "answer"
    ground_truth_method: "use_existing_labels"
    description: "Autonomous agent scenarios with pre-labeled hallucinations"
    has_labels: true  # Dataset already has hallucination_label column
    
  psychology:
    name: "TruthfulQA"
    domain: "Psychology"
    file_path: "data/raw/TruthfulQA.csv"
    prompt_column: "Question"
    answer_column: "Best Answer"
    ground_truth_method: "truthfulness_label"
    description: "Questions testing cognitive biases and common misconceptions"

# Global settings
global:
  processed_dir: "data/processed"
  features_dir: "data/features"
  train_test_split: 0.8
  random_seed: 42
  min_samples_per_domain: 500
