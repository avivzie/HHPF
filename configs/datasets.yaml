# Dataset-specific configuration for HHPF

datasets:
  medicine:
    name: "Med-HALT"
    domain: "Medicine"
    file_path: "data/raw/med_halt.csv"
    prompt_column: "question"
    answer_column: "correct_answer"
    ground_truth_method: "medical_entity_matching"
    description: "Clinical reasoning questions with ground truth medical answers"
    
  math:
    name: "GSM8K"
    domain: "Math"
    file_path: "data/raw/gsm8k.csv"
    prompt_column: "question"
    answer_column: "answer"
    ground_truth_method: "exact_numerical_match"
    description: "Grade school math word problems with numerical answers"
    
  finance:
    name: "TAT-QA"
    domain: "Finance"
    file_path: "data/raw/tatqa.csv"
    prompt_column: "question"
    answer_column: "answer"
    documents_column: "context"
    ground_truth_method: "document_grounded"
    description: "Tabular and textual financial Q&A with source documents (tables + paragraphs)"
    has_labels: false  # Labels computed by comparing LLM response to source documents
    
  is_agents:
    name: "HalluMix"
    domain: "IS"
    file_path: "data/raw/hallumix.csv"
    prompt_column: "question"
    answer_column: "answer"
    documents_column: "documents"
    ground_truth_method: "document_grounded"
    description: "Document-grounded QA with hallucination detection against source documents"
    has_labels: false  # Labels are now computed by comparing LLM response to source documents
    
  psychology:
    name: "TruthfulQA"
    domain: "Psychology"
    file_path: "data/raw/TruthfulQA.csv"
    prompt_column: "Question"
    answer_column: "Best Answer"
    ground_truth_method: "truthfulness_label"
    description: "Questions testing cognitive biases and common misconceptions"

# Global settings
global:
  processed_dir: "data/processed"
  features_dir: "data/features"
  train_test_split: 0.8
  random_seed: 42
  min_samples_per_domain: 500
