{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HHPF Data Exploration\n",
    "\n",
    "This notebook explores the datasets and demonstrates the HHPF pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.utils import load_config\n",
    "from src.data_preparation.dataset_loaders import get_loader\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('datasets')\n",
    "\n",
    "print(\"Available datasets:\")\n",
    "for domain, info in config['datasets'].items():\n",
    "    print(f\"  - {domain}: {info['name']} ({info['domain']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset (Math Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with math dataset (easiest to work with)\n",
    "domain = 'math'\n",
    "\n",
    "try:\n",
    "    loader = get_loader(domain)\n",
    "    df = loader.load_dataset(domain)\n",
    "    df = loader.get_prompt_and_answer(domain, df)\n",
    "    \n",
    "    print(f\"Loaded {len(df)} examples\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    \n",
    "    # Display sample\n",
    "    df.head()\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Dataset not found: {e}\")\n",
    "    print(\"\\nPlease place your GSM8K dataset as 'data/raw/gsm8k.csv'\")\n",
    "    print(\"Expected columns: 'question', 'answer'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\n",
    "    # Prompt length distribution\n",
    "    df['prompt_length'] = df['prompt'].str.len()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Length distribution\n",
    "    axes[0].hist(df['prompt_length'], bins=30, edgecolor='black')\n",
    "    axes[0].set_xlabel('Prompt Length (characters)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Prompt Length Distribution')\n",
    "    \n",
    "    # Word count distribution\n",
    "    df['word_count'] = df['prompt'].str.split().str.len()\n",
    "    axes[1].hist(df['word_count'], bins=30, edgecolor='black', color='coral')\n",
    "    axes[1].set_xlabel('Word Count')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Prompt Word Count Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nPrompt Statistics:\")\n",
    "    print(f\"  Mean length: {df['prompt_length'].mean():.0f} characters\")\n",
    "    print(f\"  Mean words: {df['word_count'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sample Prompts and Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\n",
    "    # Display random samples\n",
    "    samples = df.sample(n=3)\n",
    "    \n",
    "    for idx, row in samples.iterrows():\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Example {idx + 1}\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Prompt: {row['prompt'][:200]}...\")\n",
    "        print(f\"\\nGround Truth: {row['ground_truth']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process Dataset for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preparation.process_datasets import process_dataset\n",
    "\n",
    "# Process the dataset\n",
    "try:\n",
    "    processed_df = process_dataset(domain='math')\n",
    "    \n",
    "    print(\"\\nâœ“ Dataset processed successfully!\")\n",
    "    print(f\"  Saved to: data/processed/math_processed.csv\")\n",
    "    print(f\"  Total samples: {len(processed_df)}\")\n",
    "    print(f\"  Train samples: {(processed_df['split'] == 'train').sum()}\")\n",
    "    print(f\"  Test samples: {(processed_df['split'] == 'test').sum()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "\n",
    "Now that the dataset is processed, you can:\n",
    "\n",
    "1. **Generate responses**: Run `python -m src.inference.response_generator --dataset math --limit 100` (start with small sample)\n",
    "2. **Extract features**: Run `python -m src.features.feature_aggregator --responses data/features/responses_math_processed.csv --output data/features/math_features.csv`\n",
    "3. **Train model**: Run `python -m src.classifier.xgboost_model --features data/features/math_features.csv`\n",
    "\n",
    "Or use the end-to-end pipeline:\n",
    "```bash\n",
    "python run_pipeline.py --domain math --limit 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cost Estimation\n",
    "\n",
    "Estimate API costs before running inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'processed_df' in locals():\n",
    "    n_prompts = len(processed_df)\n",
    "    n_samples = 10  # Stochastic samples per prompt\n",
    "    avg_tokens = 500  # Estimated average\n",
    "    \n",
    "    total_tokens = n_prompts * n_samples * avg_tokens\n",
    "    \n",
    "    # Together AI pricing\n",
    "    cost_8b = (total_tokens / 1_000_000) * 0.20  # $0.20 per 1M tokens\n",
    "    cost_70b = (total_tokens / 1_000_000) * 0.88  # $0.88 per 1M tokens\n",
    "    \n",
    "    print(\"Estimated API Costs:\")\n",
    "    print(f\"  Prompts: {n_prompts:,}\")\n",
    "    print(f\"  Total tokens: {total_tokens:,}\")\n",
    "    print(f\"  Llama-3-8B: ${cost_8b:.2f}\")\n",
    "    print(f\"  Llama-3-70B: ${cost_70b:.2f}\")\n",
    "    print(f\"\\nRecommendation: Start with 8B model (cheaper, faster)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
